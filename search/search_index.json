{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to Datashop Datashop is a platform developed for data scientists, machine learning researcher and students to publish their work as a service. Register as user Register as user to get access all the services published on Datashop. click here to signup select \"user\" as account type and fill in your details sign in with your user account to subscribe to the services and create your work flow Register as developer Register as developer to get access to developing your own service for Datashop. click here to signup select \"developer\" as account type and fill in your details sign in with your developer account to create your service and publish it on Datashop.","title":"Home"},{"location":"index.html#welcome-to-datashop","text":"Datashop is a platform developed for data scientists, machine learning researcher and students to publish their work as a service.","title":"Welcome to Datashop"},{"location":"index.html#register-as-user","text":"Register as user to get access all the services published on Datashop. click here to signup select \"user\" as account type and fill in your details sign in with your user account to subscribe to the services and create your work flow","title":"Register as user"},{"location":"index.html#register-as-developer","text":"Register as developer to get access to developing your own service for Datashop. click here to signup select \"developer\" as account type and fill in your details sign in with your developer account to create your service and publish it on Datashop.","title":"Register as developer"},{"location":"Datashopdashboard.html","text":"Datashop Dashboard Live Dashboard live dashboard displays stats about the jobs and services offered by the Datashop platform. It is a good way to get an overview of newly added services and recently finished jobs. it also displays recent developers added to the platform. Profile Update your profile to let other users know more about yourself. click on submit to save your changes. Developers Developers page will display a list of all developers in the platform. You can click on the view button next developer to see their profile in detail Data Manager In the data manager tab you can manage your data. You can add new data sets, delete data sets and also create job requests to process your data. Add new data set follow the steps below: click on \"Upload Data\" choose a file to upload give a name to the datafile a description of the datafile you also have an option to create new job using \"Create Job\" next to the datafile. Service Manager service manager will display the list of all services in the platform. You can click on the view button next service to read about the service in detail. \"developers\" can also remove their services by clicking on the \"delete\" button next to the service. create a new service details require to create a service: name of the service URL link to the service A detail description of the service service cost input requirements to the access the service Job Manager Job manager will display the list of all jobs run by the user. You can click on the view button next job to display the job results. the \"download\" button will download results (current we support json/images/csv/.zip files as output). user can also create jobs by clicking on the \"create job\" button and selecting a service they like. create a new job details require to create a job: name of the job select a service from the list select type of input data to the service URL link to the data file (\"The URL should be publicly accessible\")","title":"Datashop dashboard"},{"location":"Datashopdashboard.html#datashop-dashboard","text":"","title":"Datashop Dashboard"},{"location":"Datashopdashboard.html#live-dashboard","text":"live dashboard displays stats about the jobs and services offered by the Datashop platform. It is a good way to get an overview of newly added services and recently finished jobs. it also displays recent developers added to the platform.","title":"Live Dashboard"},{"location":"Datashopdashboard.html#profile","text":"Update your profile to let other users know more about yourself. click on submit to save your changes.","title":"Profile"},{"location":"Datashopdashboard.html#developers","text":"Developers page will display a list of all developers in the platform. You can click on the view button next developer to see their profile in detail","title":"Developers"},{"location":"Datashopdashboard.html#data-manager","text":"In the data manager tab you can manage your data. You can add new data sets, delete data sets and also create job requests to process your data.","title":"Data Manager"},{"location":"Datashopdashboard.html#add-new-data-set","text":"follow the steps below: click on \"Upload Data\" choose a file to upload give a name to the datafile a description of the datafile you also have an option to create new job using \"Create Job\" next to the datafile.","title":"Add new data set"},{"location":"Datashopdashboard.html#service-manager","text":"service manager will display the list of all services in the platform. You can click on the view button next service to read about the service in detail. \"developers\" can also remove their services by clicking on the \"delete\" button next to the service.","title":"Service Manager"},{"location":"Datashopdashboard.html#create-a-new-service","text":"details require to create a service: name of the service URL link to the service A detail description of the service service cost input requirements to the access the service","title":"create a new service"},{"location":"Datashopdashboard.html#job-manager","text":"Job manager will display the list of all jobs run by the user. You can click on the view button next job to display the job results. the \"download\" button will download results (current we support json/images/csv/.zip files as output). user can also create jobs by clicking on the \"create job\" button and selecting a service they like.","title":"Job Manager"},{"location":"Datashopdashboard.html#create-a-new-job","text":"details require to create a job: name of the job select a service from the list select type of input data to the service URL link to the data file (\"The URL should be publicly accessible\")","title":"create a new job"},{"location":"installation.html","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"installation.html#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"installation.html#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"installation.html#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"newservice/Flasktemplate.html","text":"Deploying an ML Service using Flask and docker for datashop to begin with we recommend you watch our video tutorial on Part 1 Software requirements: docker python 3.+ Clone the template click the following link service template and clone the repository to your local machine git clone https://github.com/manaspalaparthi/service-template.git Template sturcture service-template \u251c\u2500\u2500 Datashop \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 preprocess.py \u2502 \u251c\u2500\u2500 postproces.py \u2502 \u2514\u2500\u2500 backend.py \u251c\u2500\u2500 app.py \u251c\u2500\u2500 main.py \u251c\u2500\u2500 service.py \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 model \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 model.py Out of all the files in the template, only the following files are required to be modified: main.py (update job status) service.py (actual service itself) Prepare your model Create a new directory in the service-template repository called model if not exists copy your model file to the model directory create a init .py file in the model directory if not exists Initalize your model in init .py file as shown below example model / init .py file : import tensorflow as tf # load model try: model = tf.keras.models.load_model(\"model/EfficientNetB0/\") print(\"Model loaded\") except: print(\"Model not found\") Write your Inference script To write your inference script, please use service.py file from the root folder of the service-template repository. all the code you write in service.py file will be executed when the service is called. user input data such as images/ csv / json are saved in \"tmp\" folder of the service-template repository. read the documentation of \"service.run\" function to know how to read the data from the \"tmp\" folder. title:: run description:: Run the model/get the predictions according the service. inputs:: jobID Job ID from datashop application used for search file or save file returns:: insightsDataFileLocation load data from temp folder > json data is data.json > all images and CSV are named with jobID_\"filetype\" > jobiD_csv.csv \"61ef72ed396fc5330c15f250_csv.csv\" > jobiD_image.png \"61ef72ed396fc5330c15f250_image.png\" to read the data you can use the following code: # reading images with file name jobID-image.png or jobID-image.jpg or jobID-image.jpeg # 61ef72ed396fc5330c15f250_image.png or 61ef72ed396fc5330c15f250_image.jpg or 61ef72ed396fc5330c15f250_image.jpeg (inspecific) fileslist = glob.glob(os.getcwd()+\"/tmp/\"+jobID+\"-image\"+\"*\") Sample run function: in this example we are searching for image files in tmp folder with the job-ID and performing inference on the image. def run(jobID): print(\"************************ \\n\\n excuting jobID:\"+ str(jobID)+\" \\n\\n\\n\") \"\"\" title:: run description:: Run the model/get the predictions according the service. inputs:: jobID Job ID from datashop application used for search file or save file returns:: insightsDataFileLocation load data from temp folder > json data is data.json > all images and CSV are named with jobID_\"filetype\" > jobiD_csv.csv \"61ef72ed396fc5330c15f250_csv.csv\" > jobiD_image.png \"61ef72ed396fc5330c15f250_image.png\" \"\"\" fileslist = glob.glob(os.getcwd()+\"/tmp/\"+jobID+\"-image\"+\"*\") #load image as numpy array img = cv2.imread(fileslist[0]) img = cv2.resize(img, (224, 224)) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = np.expand_dims(img, axis=0) result = model.predict(img) if result[0][0] > 0.5: result = { \"Autistic\": \"Positive\", \"confidence\":str(result[0][0]) } else: result = { \"Autistic\": \"Negative\", \"confidence\": str(result[0][0]) } print(\"model inference finished!\", str(result)) return [result] Translate your model results into insights by converting them to Json / image / csv / graphs and return them as a list. we support multiple insights as a list. ( Json is default on 0th location of the list ) Handle multiple results (comming soon) Test your model To test the service, please run python app.py The service will be up and running on \"http://localhost:5000/predict\" or \"http://127.0.0.1:5000/predict\" we have provided a sample payload for the service in the root folder of the service-template repository. Use Postman to test the service as shown in the video.","title":"step 1 - Edit service"},{"location":"newservice/Flasktemplate.html#deploying-an-ml-service-using-flask-and-docker-for-datashop","text":"to begin with we recommend you watch our video tutorial on Part 1 Software requirements: docker python 3.+","title":"Deploying an ML Service using Flask and docker for datashop"},{"location":"newservice/Flasktemplate.html#clone-the-template","text":"click the following link service template and clone the repository to your local machine git clone https://github.com/manaspalaparthi/service-template.git","title":"Clone the template"},{"location":"newservice/Flasktemplate.html#template-sturcture","text":"service-template \u251c\u2500\u2500 Datashop \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 preprocess.py \u2502 \u251c\u2500\u2500 postproces.py \u2502 \u2514\u2500\u2500 backend.py \u251c\u2500\u2500 app.py \u251c\u2500\u2500 main.py \u251c\u2500\u2500 service.py \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 model \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 model.py Out of all the files in the template, only the following files are required to be modified: main.py (update job status) service.py (actual service itself)","title":"Template sturcture"},{"location":"newservice/Flasktemplate.html#prepare-your-model","text":"Create a new directory in the service-template repository called model if not exists copy your model file to the model directory create a init .py file in the model directory if not exists Initalize your model in init .py file as shown below example model / init .py file : import tensorflow as tf # load model try: model = tf.keras.models.load_model(\"model/EfficientNetB0/\") print(\"Model loaded\") except: print(\"Model not found\")","title":"Prepare your model"},{"location":"newservice/Flasktemplate.html#write-your-inference-script","text":"To write your inference script, please use service.py file from the root folder of the service-template repository. all the code you write in service.py file will be executed when the service is called. user input data such as images/ csv / json are saved in \"tmp\" folder of the service-template repository. read the documentation of \"service.run\" function to know how to read the data from the \"tmp\" folder. title:: run description:: Run the model/get the predictions according the service. inputs:: jobID Job ID from datashop application used for search file or save file returns:: insightsDataFileLocation load data from temp folder > json data is data.json > all images and CSV are named with jobID_\"filetype\" > jobiD_csv.csv \"61ef72ed396fc5330c15f250_csv.csv\" > jobiD_image.png \"61ef72ed396fc5330c15f250_image.png\" to read the data you can use the following code: # reading images with file name jobID-image.png or jobID-image.jpg or jobID-image.jpeg # 61ef72ed396fc5330c15f250_image.png or 61ef72ed396fc5330c15f250_image.jpg or 61ef72ed396fc5330c15f250_image.jpeg (inspecific) fileslist = glob.glob(os.getcwd()+\"/tmp/\"+jobID+\"-image\"+\"*\") Sample run function: in this example we are searching for image files in tmp folder with the job-ID and performing inference on the image. def run(jobID): print(\"************************ \\n\\n excuting jobID:\"+ str(jobID)+\" \\n\\n\\n\") \"\"\" title:: run description:: Run the model/get the predictions according the service. inputs:: jobID Job ID from datashop application used for search file or save file returns:: insightsDataFileLocation load data from temp folder > json data is data.json > all images and CSV are named with jobID_\"filetype\" > jobiD_csv.csv \"61ef72ed396fc5330c15f250_csv.csv\" > jobiD_image.png \"61ef72ed396fc5330c15f250_image.png\" \"\"\" fileslist = glob.glob(os.getcwd()+\"/tmp/\"+jobID+\"-image\"+\"*\") #load image as numpy array img = cv2.imread(fileslist[0]) img = cv2.resize(img, (224, 224)) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = np.expand_dims(img, axis=0) result = model.predict(img) if result[0][0] > 0.5: result = { \"Autistic\": \"Positive\", \"confidence\":str(result[0][0]) } else: result = { \"Autistic\": \"Negative\", \"confidence\": str(result[0][0]) } print(\"model inference finished!\", str(result)) return [result] Translate your model results into insights by converting them to Json / image / csv / graphs and return them as a list. we support multiple insights as a list. ( Json is default on 0th location of the list )","title":"Write your Inference script"},{"location":"newservice/Flasktemplate.html#handle-multiple-results","text":"(comming soon)","title":"Handle multiple results"},{"location":"newservice/Flasktemplate.html#test-your-model","text":"To test the service, please run python app.py The service will be up and running on \"http://localhost:5000/predict\" or \"http://127.0.0.1:5000/predict\" we have provided a sample payload for the service in the root folder of the service-template repository. Use Postman to test the service as shown in the video.","title":"Test your model"},{"location":"newservice/builddockerimage.html","text":"Building a Docker Image and pushing it to Docker Hub One the service is tested locally, now its time build a Docker image and push it to Docker Hub we recommend you watch our video tutorial on Part 2 follow the steps below: make sure your packages are up to date in requirements.txt file pip freeze > requirements.txt RUN Command to build DOCKER image docker build -t servicename . RUN Command to RUN DOCKER container docker run -d -p 5000:5000 servicename Test the service locally using docker Test your service by running thought postman Method : POST URL : http://localhost:5000/predict Body type: JSON sample payload: { \"jobID\" : \"61ef72ed396fc5330c15f250\", \"dataFileURL\": { \"url\": \"\", \"json\":\"\" }, \"datashopServerAddress\": \"http://thedatashop.club:8000\" } Push the image to Docker Hub ones the image is built and tested locally, now its time push it to Docker Hub register to Docker Hub and login with your credentials (see here ) use: docker login to login to Docker Hub docker login -u username -p password step 1: create a new repository on Docker Hub ones the repository is created, link the image with the repository name using \"docker tag\" command docker tag servicename:latest username/servicename:latest step 2: push the image to Docker Hub docker push username/servicename:latest in short time the image is pushed to Docker Hub and is available to pull","title":"step 2 - Build docker image"},{"location":"newservice/builddockerimage.html#building-a-docker-image-and-pushing-it-to-docker-hub","text":"One the service is tested locally, now its time build a Docker image and push it to Docker Hub we recommend you watch our video tutorial on Part 2 follow the steps below: make sure your packages are up to date in requirements.txt file pip freeze > requirements.txt RUN Command to build DOCKER image docker build -t servicename . RUN Command to RUN DOCKER container docker run -d -p 5000:5000 servicename","title":"Building a Docker Image and pushing it to Docker Hub"},{"location":"newservice/builddockerimage.html#test-the-service-locally-using-docker","text":"Test your service by running thought postman Method : POST URL : http://localhost:5000/predict Body type: JSON sample payload: { \"jobID\" : \"61ef72ed396fc5330c15f250\", \"dataFileURL\": { \"url\": \"\", \"json\":\"\" }, \"datashopServerAddress\": \"http://thedatashop.club:8000\" }","title":"Test the service locally using docker"},{"location":"newservice/builddockerimage.html#push-the-image-to-docker-hub","text":"ones the image is built and tested locally, now its time push it to Docker Hub register to Docker Hub and login with your credentials (see here ) use: docker login to login to Docker Hub docker login -u username -p password","title":"Push the image to Docker Hub"},{"location":"newservice/builddockerimage.html#step-1-create-a-new-repository-on-docker-hub","text":"ones the repository is created, link the image with the repository name using \"docker tag\" command docker tag servicename:latest username/servicename:latest","title":"step 1: create a new repository on Docker Hub"},{"location":"newservice/builddockerimage.html#step-2-push-the-image-to-docker-hub","text":"docker push username/servicename:latest in short time the image is pushed to Docker Hub and is available to pull","title":"step 2: push the image to Docker Hub"},{"location":"newservice/createservice.html","text":"Creating new service to register for DataShop Datashop supports different types of Machine learning deployment scenarios. we has designed service templates to help you create a service in few steps. Types of deployment scenarios: Serverless deployement : template to deploy your service using AWS Lambda, Sagemaker and API Gateway (AWS infrastructure) Flask Docker container : template to deploy your service Using Flask and docker. (deployed on cloud VM or container hosting service)","title":"Deployment methods"},{"location":"newservice/createservice.html#creating-new-service-to-register-for-datashop","text":"Datashop supports different types of Machine learning deployment scenarios. we has designed service templates to help you create a service in few steps. Types of deployment scenarios: Serverless deployement : template to deploy your service using AWS Lambda, Sagemaker and API Gateway (AWS infrastructure) Flask Docker container : template to deploy your service Using Flask and docker. (deployed on cloud VM or container hosting service)","title":"Creating new service to register for DataShop"},{"location":"newservice/deployimage.html","text":"Deploying image container on IBM Cloud Container hosting Service (code engine) Ones you have created a Docker image and pushed it to docker hub, you can now deploy it on IBM Cloud Container Service (code engine). we recommend you watch our video tutorial part 3 to get a better understanding of how to deploy an image on code engine login to IBM cloud steps to follow: click here to login to IBM cloud Code Engine deployment navigate to code engine service from the sidebar menu Go to projects and select your project or create a new project click on \"start creating\" select name of the container select project select \"choose the run to code\" as \"image container\" post the image reference click \"create\" to create the container. Wait until the status turns green and \"ready\" copy the container URL and add /predict (api route) example: https://autismdetectionservice.klm8igvn30j.au-syd.codeengine.appdomain.cloud/predict The link is the service URL, Use this to create a DataShop service from the [service manager]","title":"step 3 - Deploy the image container"},{"location":"newservice/deployimage.html#deploying-image-container-on-ibm-cloud-container-hosting-service-code-engine","text":"Ones you have created a Docker image and pushed it to docker hub, you can now deploy it on IBM Cloud Container Service (code engine). we recommend you watch our video tutorial part 3 to get a better understanding of how to deploy an image on code engine","title":"Deploying image container on IBM Cloud Container hosting Service (code engine)"},{"location":"newservice/deployimage.html#login-to-ibm-cloud","text":"steps to follow: click here to login to IBM cloud","title":"login to IBM cloud"},{"location":"newservice/deployimage.html#code-engine-deployment","text":"navigate to code engine service from the sidebar menu Go to projects and select your project or create a new project click on \"start creating\" select name of the container select project select \"choose the run to code\" as \"image container\" post the image reference click \"create\" to create the container. Wait until the status turns green and \"ready\" copy the container URL and add /predict (api route) example: https://autismdetectionservice.klm8igvn30j.au-syd.codeengine.appdomain.cloud/predict The link is the service URL, Use this to create a DataShop service from the [service manager]","title":"Code Engine deployment"},{"location":"newservice/registerservice.html","text":"Create a new service on Datashop to create a new service on the datashop, you need to register as developer and navigate to \"service manager\" tab. then click on \"createservice\" button. Fill up the following details and click on \"create service\" button. name of the service URL link to the service (IBM code engine container URL) A detail description of the service service cost input requirements to the access the service Ones the service is created, you can test the service either from data-manager or from job manager as shown in the Datashop part 3 tutorial.","title":"Register service on Datashop"},{"location":"newservice/registerservice.html#create-a-new-service-on-datashop","text":"to create a new service on the datashop, you need to register as developer and navigate to \"service manager\" tab. then click on \"createservice\" button. Fill up the following details and click on \"create service\" button. name of the service URL link to the service (IBM code engine container URL) A detail description of the service service cost input requirements to the access the service Ones the service is created, you can test the service either from data-manager or from job manager as shown in the Datashop part 3 tutorial.","title":"Create a new service on Datashop"},{"location":"newservice/serverlessdeployment.html","text":"Deploying an ML Service using Serverless Framework (AWS Lambda , SageMaker, API Gateway) COMMING SOON","title":"serverless deployment on AWS"},{"location":"newservice/serverlessdeployment.html#deploying-an-ml-service-using-serverless-framework-aws-lambda-sagemaker-api-gateway","text":"","title":"Deploying an ML Service using Serverless Framework (AWS Lambda , SageMaker, API Gateway)"},{"location":"newservice/serverlessdeployment.html#comming-soon","text":"","title":"COMMING SOON"}]}